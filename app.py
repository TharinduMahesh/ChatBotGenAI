# from flask import Flask, render_template, request, jsonify
# from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings
# from langchain_pinecone import PineconeVectorStore
# from langchain_google_genai import ChatGoogleGenerativeAI 
# from langchain.chains import create_retrieval_chain
# from langchain.chains.combine_documents import create_stuff_documents_chain
# from langchain_core.prompts import ChatPromptTemplate
# from src.prompt import *
# import os
# from dotenv import load_dotenv





# app = Flask(__name__)

# load_dotenv()




# PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
# GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# embedding = download_hugging_face_embeddings()
# index_name = "medbot"

# docsearch = PineconeVectorStore.from_existing_index(
#  index_name=index_name,
#     embedding=embedding,
#     )

# retriver = docsearch.as_retriever(search_type="similarity", search_kwargs={"k":3}   )

# llm = ChatGoogleGenerativeAI(
#     model="gemini-2.0-flash",
#     temperature=0.4,
#     max_output_tokens=500,
#     google_api_key=GOOGLE_API_KEY

# )


# question_answer_chain = create_stuff_documents_chain(llm, prompt)
# rag_chain = create_retrieval_chain(retriver, question_answer_chain)


# @app.route("/")
# def index():
#     return render_template('index.html') 
                                      
# @app.route("/get" ,methods=["GET" ,"POST"])
# def chat():
#       msg =request.form["msg"]
#       input = msg
#       print(input)
#       response = rag_chain.invoke({"input":msg})
#       print("Response : " ,response["answer"])
#       return str(response["answer"])


# if __name__ == "__main__":
#     app.run(host="0.0.0.0", port=8080, debug=True)


# app.py
# app.py
from flask import Flask, render_template, request
from src.helper import download_hugging_face_embeddings
from langchain_pinecone import PineconeVectorStore
from langchain_google_genai import ChatGoogleGenerativeAI 
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from src.prompt import prompt 
import os
from dotenv import load_dotenv

app = Flask(__name__)
load_dotenv()

PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

embeddings = download_hugging_face_embeddings()
index_name = "medbot"

# Connect to existing index
docsearch = PineconeVectorStore.from_existing_index(
    index_name=index_name,
    embedding=embeddings,
)

retriever = docsearch.as_retriever(search_type="similarity", search_kwargs={"k":3})

llm = ChatGoogleGenerativeAI(
    model="gemini-pro", # Ensure your API key has access to 2.0
    temperature=0.4,
    max_output_tokens=500,
    google_api_key=GOOGLE_API_KEY
)

# This creates the logic to handle the retrieved documents
combine_docs_chain = create_stuff_documents_chain(llm, prompt)

# This creates the full RAG chain
rag_chain = create_retrieval_chain(retriever, combine_docs_chain)

@app.route("/")
def index():
    return render_template('index.html') 
                                      
@app.route("/get", methods=["GET", "POST"])
def chat():
    try:
        msg = request.form["msg"]
        print(f"User Input: {msg}")
        
        # Generate response
        response = rag_chain.invoke({"input": msg})
        return str(response["answer"])

    except Exception as e:
        print(f"Error occurred: {e}")
        # Return a friendly message if the API is busy or quota is hit
        if "429" in str(e) or "ResourceExhausted" in str(e):
            return "The AI is currently busy (Rate Limit reached). Please try again in a few seconds."
        return "Sorry, I encountered an error processing your request."

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8080, debug=True)